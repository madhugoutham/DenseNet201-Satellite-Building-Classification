{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Preprocessing and Segmentation\n",
                "\n",
                "**Paper Reference:** Section 3.2 - Model (Segmentation Module)\n",
                "\n",
                "This notebook implements the preprocessing and building segmentation pipeline.\n",
                "\n",
                "## Overview\n",
                "\n",
                "From the paper:\n",
                "> \"After resizing each input image to 512×512 pixels and normalizing pixel intensities, images were processed using ReFineNet, a pretrained segmentation network. Post-processing further refined these masks by applying morphological opening to eliminate small artifacts and reduce noise, followed by the watershed algorithm.\"\n",
                "\n",
                "**Pipeline:**\n",
                "1. Image resizing to 512×512\n",
                "2. Pixel intensity normalization\n",
                "3. ReFineNet segmentation (pretrained)\n",
                "4. Test-time augmentation (TTA)\n",
                "5. Morphological post-processing\n",
                "6. Watershed segmentation\n",
                "7. Size filtering (500-100,000 pixels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core imports\n",
                "import os\n",
                "import numpy as np\n",
                "import cv2\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Deep learning imports\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing import image as keras_image\n",
                "\n",
                "# scipy for watershed\n",
                "from scipy import ndimage\n",
                "from skimage import morphology, measure, segmentation\n",
                "from skimage.feature import peak_local_max\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPUs available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Image specifications (Paper Section 3.2)\n",
                "IMAGE_SIZE = 512              # Target image size\n",
                "MODEL_INPUT_SIZE = 224        # DenseNet201 input size\n",
                "\n",
                "# Segmentation parameters (Paper Section 3.2)\n",
                "MIN_BUILDING_AREA = 500       # Minimum segment area in pixels\n",
                "MAX_BUILDING_AREA = 100000    # Maximum segment area in pixels\n",
                "\n",
                "# Morphological operation kernel size\n",
                "MORPH_KERNEL_SIZE = 5\n",
                "\n",
                "# Building classes\n",
                "BUILDING_CLASSES = ['Commercial', 'High', 'Hospital', 'Industrial', 'Multi', 'Schools', 'Single']\n",
                "\n",
                "print(f\"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
                "print(f\"Building Area Range: {MIN_BUILDING_AREA}-{MAX_BUILDING_AREA} pixels\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Image Preprocessing\n",
                "\n",
                "Paper Section 3.2:\n",
                "> \"After resizing each input image to 512×512 pixels and normalizing pixel intensities...\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_and_preprocess_image(image_path, target_size=IMAGE_SIZE):\n",
                "    \"\"\"\n",
                "    Load and preprocess satellite image for segmentation.\n",
                "    \n",
                "    Paper Reference: Section 3.2\n",
                "    \"After resizing each input image to 512×512 pixels and \n",
                "     normalizing pixel intensities...\"\n",
                "    \n",
                "    Args:\n",
                "        image_path (str): Path to input image\n",
                "        target_size (int): Target image size\n",
                "        \n",
                "    Returns:\n",
                "        np.array: Preprocessed image (normalized, resized)\n",
                "    \"\"\"\n",
                "    # Load image\n",
                "    img = cv2.imread(str(image_path))\n",
                "    if img is None:\n",
                "        raise ValueError(f\"Could not load image: {image_path}\")\n",
                "    \n",
                "    # Convert BGR to RGB\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    # Resize to target size\n",
                "    img = cv2.resize(img, (target_size, target_size))\n",
                "    \n",
                "    # Normalize pixel intensities to [0, 1]\n",
                "    img_normalized = img.astype(np.float32) / 255.0\n",
                "    \n",
                "    return img_normalized\n",
                "\n",
                "def prepare_for_model(img, model_input_size=MODEL_INPUT_SIZE):\n",
                "    \"\"\"\n",
                "    Prepare image for DenseNet201 classification.\n",
                "    \n",
                "    Args:\n",
                "        img (np.array): Input image\n",
                "        model_input_size (int): Model input size (224 for DenseNet)\n",
                "        \n",
                "    Returns:\n",
                "        np.array: Image ready for model prediction\n",
                "    \"\"\"\n",
                "    # Resize for model input\n",
                "    img_resized = cv2.resize(img, (model_input_size, model_input_size))\n",
                "    \n",
                "    # Add batch dimension\n",
                "    img_batch = np.expand_dims(img_resized, axis=0)\n",
                "    \n",
                "    return img_batch\n",
                "\n",
                "print(\"Preprocessing functions defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Test-Time Augmentation (TTA)\n",
                "\n",
                "Paper Section 3.2:\n",
                "> \"To further improve mask robustness against variations in building orientation and appearance, we employed test-time augmentation (TTA). TTA involved generating predictions from horizontally and vertically flipped versions of each image and averaging these predictions.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_tta(image):\n",
                "    \"\"\"\n",
                "    Apply Test-Time Augmentation (TTA) for robust predictions.\n",
                "    \n",
                "    Paper Reference: Section 3.2\n",
                "    \"TTA involved generating predictions from horizontally and vertically \n",
                "     flipped versions of each image and averaging these predictions.\"\n",
                "    \n",
                "    Args:\n",
                "        image (np.array): Input image\n",
                "        \n",
                "    Returns:\n",
                "        list: List of augmented images [original, h_flip, v_flip, hv_flip]\n",
                "    \"\"\"\n",
                "    augmented = [\n",
                "        image,                                    # Original\n",
                "        np.fliplr(image),                        # Horizontal flip\n",
                "        np.flipud(image),                        # Vertical flip\n",
                "        np.flipud(np.fliplr(image))              # Both flips\n",
                "    ]\n",
                "    \n",
                "    return augmented\n",
                "\n",
                "def average_tta_predictions(predictions_list):\n",
                "    \"\"\"\n",
                "    Average predictions from TTA-augmented images.\n",
                "    \n",
                "    Args:\n",
                "        predictions_list (list): List of prediction arrays\n",
                "        \n",
                "    Returns:\n",
                "        np.array: Averaged predictions\n",
                "    \"\"\"\n",
                "    return np.mean(predictions_list, axis=0)\n",
                "\n",
                "print(\"TTA functions defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Morphological Post-Processing\n",
                "\n",
                "Paper Section 3.2:\n",
                "> \"Post-processing further refined these masks by applying morphological opening to eliminate small artifacts and reduce noise.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def morphological_postprocess(mask, kernel_size=MORPH_KERNEL_SIZE):\n",
                "    \"\"\"\n",
                "    Apply morphological operations to refine segmentation mask.\n",
                "    \n",
                "    Paper Reference: Section 3.2\n",
                "    \"Post-processing further refined these masks by applying morphological \n",
                "     opening to eliminate small artifacts and reduce noise.\"\n",
                "    \n",
                "    Args:\n",
                "        mask (np.array): Binary segmentation mask\n",
                "        kernel_size (int): Size of morphological kernel\n",
                "        \n",
                "    Returns:\n",
                "        np.array: Refined binary mask\n",
                "    \"\"\"\n",
                "    # Create elliptical kernel for morphological operations\n",
                "    kernel = cv2.getStructuringElement(\n",
                "        cv2.MORPH_ELLIPSE, \n",
                "        (kernel_size, kernel_size)\n",
                "    )\n",
                "    \n",
                "    # Morphological opening: erosion followed by dilation\n",
                "    # Removes small artifacts while preserving larger structures\n",
                "    mask_opened = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
                "    \n",
                "    # Optional: closing to fill small holes\n",
                "    mask_closed = cv2.morphologyEx(mask_opened, cv2.MORPH_CLOSE, kernel)\n",
                "    \n",
                "    return mask_closed.astype(np.uint8)\n",
                "\n",
                "print(\"Morphological post-processing function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.6 Watershed Segmentation\n",
                "\n",
                "Paper Section 3.2:\n",
                "> \"...followed by the watershed algorithm, chosen for its efficacy in segmenting connected or overlapping building structures.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def watershed_segmentation(mask, image):\n",
                "    \"\"\"\n",
                "    Apply watershed algorithm to separate connected buildings.\n",
                "    \n",
                "    Paper Reference: Section 3.2 (Meyer, 1994)\n",
                "    \"...followed by the watershed algorithm, chosen for its efficacy in \n",
                "     segmenting connected or overlapping building structures.\"\n",
                "    \n",
                "    Args:\n",
                "        mask (np.array): Binary segmentation mask\n",
                "        image (np.array): Original image for visualization\n",
                "        \n",
                "    Returns:\n",
                "        np.array: Labeled segments array\n",
                "    \"\"\"\n",
                "    # Compute distance transform\n",
                "    distance = ndimage.distance_transform_edt(mask)\n",
                "    \n",
                "    # Find local maxima as markers\n",
                "    local_max = peak_local_max(\n",
                "        distance, \n",
                "        min_distance=20,\n",
                "        labels=mask\n",
                "    )\n",
                "    \n",
                "    # Create markers for watershed\n",
                "    markers = np.zeros(distance.shape, dtype=bool)\n",
                "    markers[tuple(local_max.T)] = True\n",
                "    markers = measure.label(markers)\n",
                "    \n",
                "    # Apply watershed\n",
                "    labels = segmentation.watershed(-distance, markers, mask=mask)\n",
                "    \n",
                "    return labels\n",
                "\n",
                "print(\"Watershed segmentation function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.7 Building Segment Extraction\n",
                "\n",
                "Paper Section 3.2:\n",
                "> \"We filtered segmented regions by size, retaining only those within a pixel area range of 500–100,000 pixels.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_building_segments(labels, image, \n",
                "                              min_area=MIN_BUILDING_AREA, \n",
                "                              max_area=MAX_BUILDING_AREA):\n",
                "    \"\"\"\n",
                "    Extract individual building segments within size constraints.\n",
                "    \n",
                "    Paper Reference: Section 3.2\n",
                "    \"We filtered segmented regions by size, retaining only those within \n",
                "     a pixel area range of 500–100,000 pixels.\"\n",
                "    \n",
                "    Args:\n",
                "        labels (np.array): Labeled segments from watershed\n",
                "        image (np.array): Original image\n",
                "        min_area (int): Minimum building area in pixels\n",
                "        max_area (int): Maximum building area in pixels\n",
                "        \n",
                "    Returns:\n",
                "        list: List of dictionaries with building crops and metadata\n",
                "    \"\"\"\n",
                "    buildings = []\n",
                "    \n",
                "    for region in measure.regionprops(labels):\n",
                "        # Filter by area\n",
                "        if min_area <= region.area <= max_area:\n",
                "            # Get bounding box\n",
                "            minr, minc, maxr, maxc = region.bbox\n",
                "            \n",
                "            # Extract building crop\n",
                "            building_crop = image[minr:maxr, minc:maxc]\n",
                "            \n",
                "            buildings.append({\n",
                "                'crop': building_crop,\n",
                "                'bbox': (minr, minc, maxr, maxc),\n",
                "                'area': region.area,\n",
                "                'centroid': region.centroid\n",
                "            })\n",
                "    \n",
                "    return buildings\n",
                "\n",
                "print(f\"Building extraction function defined.\")\n",
                "print(f\"Area filter: {MIN_BUILDING_AREA} - {MAX_BUILDING_AREA} pixels\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.8 Complete Segmentation Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def segment_buildings(image_path, segmentation_model=None):\n",
                "    \"\"\"\n",
                "    Complete building segmentation pipeline.\n",
                "    \n",
                "    Paper Reference: Section 3.2\n",
                "    Full pipeline as described in methodology.\n",
                "    \n",
                "    Args:\n",
                "        image_path (str): Path to satellite image\n",
                "        segmentation_model: Pretrained segmentation model (ReFineNet)\n",
                "        \n",
                "    Returns:\n",
                "        dict: Segmentation results with buildings and visualization\n",
                "    \"\"\"\n",
                "    # Step 1: Load and preprocess\n",
                "    image = load_and_preprocess_image(image_path)\n",
                "    \n",
                "    # Step 2: Apply TTA\n",
                "    tta_images = apply_tta(image)\n",
                "    \n",
                "    # Step 3: Get segmentation masks\n",
                "    # Note: Using simple threshold as placeholder for ReFineNet\n",
                "    # In production, use actual ReFineNet model\n",
                "    gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
                "    _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
                "    \n",
                "    # Step 4: Morphological post-processing\n",
                "    mask_refined = morphological_postprocess(mask)\n",
                "    \n",
                "    # Step 5: Watershed segmentation\n",
                "    labels = watershed_segmentation(mask_refined, image)\n",
                "    \n",
                "    # Step 6: Extract buildings\n",
                "    buildings = extract_building_segments(labels, image)\n",
                "    \n",
                "    return {\n",
                "        'original': image,\n",
                "        'mask': mask_refined,\n",
                "        'labels': labels,\n",
                "        'buildings': buildings,\n",
                "        'num_buildings': len(buildings)\n",
                "    }\n",
                "\n",
                "print(\"Complete segmentation pipeline defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.9 Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_segmentation(results, save_path=None):\n",
                "    \"\"\"\n",
                "    Visualize segmentation results (Figure 5 in paper).\n",
                "    \n",
                "    Paper Reference: Figure 5 - Visual Comparison of Segmentation Stages\n",
                "    \"\"\"\n",
                "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "    \n",
                "    # Original image\n",
                "    axes[0].imshow(results['original'])\n",
                "    axes[0].set_title('Original Image')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Segmentation mask\n",
                "    axes[1].imshow(results['mask'], cmap='gray')\n",
                "    axes[1].set_title('Segmentation Mask')\n",
                "    axes[1].axis('off')\n",
                "    \n",
                "    # Labeled regions\n",
                "    axes[2].imshow(results['labels'], cmap='nipy_spectral')\n",
                "    axes[2].set_title(f'Watershed Labels\\n({results[\"num_buildings\"]} buildings)')\n",
                "    axes[2].axis('off')\n",
                "    \n",
                "    # Overlay\n",
                "    overlay = results['original'].copy()\n",
                "    for building in results['buildings']:\n",
                "        minr, minc, maxr, maxc = building['bbox']\n",
                "        cv2.rectangle(overlay, (minc, minr), (maxc, maxr), (0, 1, 0), 2)\n",
                "    axes[3].imshow(overlay)\n",
                "    axes[3].set_title('Detected Buildings')\n",
                "    axes[3].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    \n",
                "    if save_path:\n",
                "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "    \n",
                "    plt.show()\n",
                "\n",
                "print(\"Visualization function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook implements the preprocessing and segmentation pipeline:\n",
                "\n",
                "1. **Image Preprocessing**: Resize to 512×512, normalize pixel intensities\n",
                "2. **Test-Time Augmentation**: H/V flips for robust predictions\n",
                "3. **Morphological Refinement**: Opening to remove artifacts\n",
                "4. **Watershed Segmentation**: Separate overlapping buildings\n",
                "5. **Size Filtering**: Keep buildings with 500-100,000 pixel area\n",
                "\n",
                "**Next Step**: `03_model_training.ipynb` - Train DenseNet201 classifier"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}